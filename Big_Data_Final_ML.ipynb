{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data Final ML",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMjp22biUww8"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown"
      ],
      "metadata": {
        "id": "BEiFtxb_rsvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ~/.cache/gdown/cookies.json\n",
        "# Download all data unzipped\n",
        "!gdown --id 1-VDT520b2ApUSttqd1aKc5bYAUiJX1zE   # Oct19\n",
        "#!gdown --id 1-ThB1vQbQHgt3Am3FGw_F0tf5h_3K5Xs   # Nov19\n",
        "!gdown --id 1-6gv2iI4-4BFcw5FD5xi3kXsBhtLHl9p   # Dec19\n",
        "!gdown --id 1-LzWyIcna1rkbgYCXxHYlABtN8vZpeE2   # Jan20\n",
        "!gdown --id 1-Em1YaZq13OdbZbu3XHAmIXSgEBNYcSF   # Feb20\n",
        "!gdown --id 1-RHOXC5IoCGDSMukI5uPS4R2XZedty1i   # Mar20\n",
        "!gdown --id 1-D_V71xZY2WCr_RUqYsuEu3hzjpMeyAd   # Apr20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJuFfGoIVZvz",
        "outputId": "88a98124-0f96-4023-86c6-c753c9f379ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/root/.cache/gdown/cookies.json': No such file or directory\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-VDT520b2ApUSttqd1aKc5bYAUiJX1zE\n",
            "To: /content/2019-Oct.csv\n",
            "100% 5.67G/5.67G [01:44<00:00, 54.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-6gv2iI4-4BFcw5FD5xi3kXsBhtLHl9p\n",
            "To: /content/2019-Dec.csv\n",
            "100% 9.36G/9.36G [03:33<00:00, 43.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-LzWyIcna1rkbgYCXxHYlABtN8vZpeE2\n",
            "To: /content/2020-Jan.csv\n",
            "100% 7.78G/7.78G [01:48<00:00, 71.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-Em1YaZq13OdbZbu3XHAmIXSgEBNYcSF\n",
            "To: /content/2020-Feb.csv\n",
            "100% 7.68G/7.68G [02:03<00:00, 62.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-RHOXC5IoCGDSMukI5uPS4R2XZedty1i\n",
            "To: /content/2020-Mar.csv\n",
            "100% 7.82G/7.82G [02:06<00:00, 62.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-D_V71xZY2WCr_RUqYsuEu3hzjpMeyAd\n",
            "To: /content/2020-Apr.csv\n",
            "100% 9.27G/9.27G [02:32<00:00, 60.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob as g\n",
        "paths = g.glob(\"*.csv\", recursive=True)\n",
        "paths"
      ],
      "metadata": {
        "id": "r4oqsUs5lSNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1325ebed-602a-409d-c936-25c9a0ad02b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020-Apr.csv',\n",
              " '2019-Oct.csv',\n",
              " '2019-Dec.csv',\n",
              " '2020-Mar.csv',\n",
              " '2020-Jan.csv',\n",
              " '2020-Feb.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler, OneHotEncoder"
      ],
      "metadata": {
        "id": "m6tQeOgjlC3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating spark context\n",
        "sc = SparkContext()\n",
        "# Create a spark session\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "dagGncjRlNe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\",True).csv(paths)\n",
        "df_corrected=df.withColumn(\"category_code\",when(col(\"category_code\")==\"construction.tools.light\",\"electronics.smartphone\").otherwise(col(\"category_code\")))\n",
        "df_targets = df_corrected.filter(df_corrected[\"event_type\"].isin('cart','purchase')).drop_duplicates(subset=['event_type', 'product_id','price', 'user_id','user_session']) # select only cart and purchase events\n",
        "\n",
        "df_targets = df_targets.dropna(how='any')\n",
        "\n",
        "df_targets = df_targets.withColumn(\"is_purchased\", (df_targets[\"event_type\"]==\"purchase\").cast(\"int\")) # add boolean based on event type, 1 for purchase, 0 otherwise\n",
        "df_targets = df_targets.withColumn(\"price\", df_targets[\"price\"].cast(\"float\"))\n",
        "df_interm = df_targets.groupby([\"user_session\",\"product_id\"]).max(\"is_purchased\") # assign the final purchase value of the user session to all the session actions\n",
        "df_interm = df_interm.withColumnRenamed(\"user_session\",\"user_session2\").withColumnRenamed(\"product_id\",\"product_id2\") # to avoid duplicate column names after join\n",
        "df_targets = df_targets.join(df_interm, (df_targets.user_session == df_interm.user_session2) & (df_targets.product_id == df_interm.product_id2)) # update is_purchased column in original DF\n",
        "df_targets = df_targets.drop(\"user_session2\").drop(\"product_id2\").drop(\"is_purchased\").withColumnRenamed(\"max(is_purchased)\",\"is_purchased\") # cleanup join result\n",
        "\n",
        "df_targets = df_targets.filter(df_targets[\"event_type\"]== 'cart').drop_duplicates([\"user_session\",\"product_id\",\"is_purchased\"]) # dropping purchase events as they're redundant\n",
        "\n",
        "weekday_func = udf(lambda s: str(datetime.strptime(str(s)[0:10], \"%Y-%m-%d\").weekday()))\n",
        "df_targets = df_targets.withColumn('event_weekday',weekday_func(col('event_time')).cast(\"int\")) # adding weekday column\n",
        "\n",
        "split_col = pyspark.sql.functions.split(df_targets[\"category_code\"], '\\.')\n",
        "df_targets = df_targets.withColumn('category_code_level1', split_col.getItem(0)).withColumn('category_code_level2', split_col.getItem(1)) # splitting category\n",
        "\n",
        "df_interm = df_targets.groupby([\"user_session\"]).count() # activity count\n",
        "df_interm = df_interm.withColumnRenamed(\"user_session\",\"user_session2\") # to avoid duplicate column names after join\n",
        "df_targets = df_targets.join(df_interm, df_targets.user_session == df_interm.user_session2)\n",
        "df_targets = df_targets.drop(\"user_session2\").withColumnRenamed(\"count\",\"activity_count\") \n",
        "df_targets = df_targets.drop(\"event_time\").drop(\"event_type\").drop(\"product_id\").drop(\"category_id\").drop(\"category_code\").drop(\"user_id\").drop(\"user_session\")"
      ],
      "metadata": {
        "id": "uBnqNtLtoiXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brand_indexer = StringIndexer(inputCol=\"brand\", outputCol=\"brand_index\").fit(df_targets)\n",
        "category_code_level1_indexer = StringIndexer(inputCol=\"category_code_level1\", outputCol=\"category_code_level1_index\")\n",
        "category_code_level2_indexer = StringIndexer(inputCol=\"category_code_level2\", outputCol=\"category_code_level2_index\")\n",
        "\n",
        "train, test = df_targets.randomSplit([0.9, 0.1], seed=12345)\n",
        "\n",
        "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
        "\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "\n",
        "vecAssembler.setInputCols([\"price\", \"event_weekday\", 'activity_count', \"brand_index\", \"category_code_level1_index\", \"category_code_level2_index\"])\n",
        "gbt = GBTClassifier(labelCol=\"is_purchased\", featuresCol=\"features\", maxIter=10, maxBins = 4000)\n",
        "indexers = [ brand_indexer, category_code_level1_indexer, category_code_level2_indexer, vecAssembler, gbt]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "\n",
        "model = pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "Ue83Ik4Iota7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_purchased\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "evaluator.setMetricName(\"weightedPrecision\")\n",
        "precision = evaluator.evaluate(predictions)\n",
        "evaluator.setMetricName(\"weightedRecall\")\n",
        "recall = evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "BtcQyfECCoYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy is {0}\\nPrecision is {1}\\nRecall is {2}'.format(accuracy,precision,recall))"
      ],
      "metadata": {
        "id": "8Nb77v0G7K_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}